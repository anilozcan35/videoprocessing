{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "import dataloader as dataloader\n",
    "import utils\n",
    "\n",
    "import time\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataloader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, t_dim=16, img_x=112, img_y=112, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=10):\n",
    "        super(CNN3D, self).__init__()\n",
    "\n",
    "        # set video dimension\n",
    "        self.t_dim = t_dim\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        \n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "        self.ch1, self.ch2 = 32, 48\n",
    "        self.k1, self.k2 = (5, 5, 5), (3, 3, 3)  # 3d kernel size\n",
    "        self.s1, self.s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\n",
    "        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 0)  # 3d padding\n",
    "\n",
    "        # compute conv1 & conv2 output shape\n",
    "        self.conv1_outshape = utils.conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)\n",
    "        self.conv2_outshape = utils.conv3D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,\n",
    "                               padding=self.pd1)\n",
    "        self.bn1 = nn.BatchNorm3d(self.ch1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,\n",
    "                               padding=self.pd2)\n",
    "        self.bn2 = nn.BatchNorm3d(self.ch2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout3d(self.drop_p)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2],\n",
    "                             self.fc_hidden1)  # fully connected hidden layer\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes)  # fully connected layer, output = multi-classes\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        # Conv 1\n",
    "        x = self.conv1(x_3d)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # Conv 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # FC 1 and 2\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN3D(\n",
      "  (conv1): Conv3d(3, 32, kernel_size=(5, 5, 5), stride=(2, 2, 2))\n",
      "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv3d(32, 48, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
      "  (bn2): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (drop): Dropout3d(p=0.2, inplace=False)\n",
      "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64896, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# Let's create an instance of the model and print its architecture\n",
    "model = CNN3D(t_dim=16, img_x=112, img_y=112, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=5)\n",
    "print(model)\n",
    "\n",
    "# Define the shape of the dummy input data (batch_size, channels, frames, height, width)\n",
    "dummy_input_shape = (32, 3, 112, 112, 16) # Batch size 32, 3 input channels, 16 frames, 112x112 resolution\n",
    "\n",
    "# Generate random dummy input data within the defined shape\n",
    "dummy_input = torch.randn(*dummy_input_shape)\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CNN3D                                    --\n",
       "├─Conv3d: 1-1                            12,032\n",
       "├─BatchNorm3d: 1-2                       64\n",
       "├─Conv3d: 1-3                            41,520\n",
       "├─BatchNorm3d: 1-4                       96\n",
       "├─ReLU: 1-5                              --\n",
       "├─Dropout3d: 1-6                         --\n",
       "├─MaxPool3d: 1-7                         --\n",
       "├─Linear: 1-8                            16,613,632\n",
       "├─Linear: 1-9                            32,896\n",
       "├─Linear: 1-10                           645\n",
       "=================================================================\n",
       "Total params: 16,700,885\n",
       "Trainable params: 16,700,885\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize dataloader\n",
    "\n",
    "# Define your transform (data pre-processing) fınction\n",
    "# Define your dataset with transform\n",
    "transform = transforms.Compose([\n",
    "    dataloader.myUCF5Preprocessing(output_size=(112, 112))\n",
    "])\n",
    "\n",
    "\n",
    "# Define your dataset\n",
    "dataset = dataloader.myUCF5Loader(root_dir='UCF5', transform=transform)\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "train_set, val_set, test_set = dataloader.split_dataset(dataset)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=2, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\citak\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Train Loss: 5.0512, Val Loss: 3.9023, Val Acc: 20.00%\n",
      "Epoch [2/3], Train Loss: 1.1597, Val Loss: 1.0611, Val Acc: 50.00%\n",
      "Epoch [3/3], Train Loss: 0.5132, Val Loss: 0.3002, Val Acc: 90.00%\n",
      "Test Loss: 0.7352, Test Acc: 80.00%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "\n",
    "# hyper-params\n",
    "num_epochs = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# dont forget to send model to device \n",
    "model.to(device) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {running_loss / len(train_loader.dataset):.4f}, \"\n",
    "          f\"Val Loss: {val_loss / len(val_loader.dataset):.4f}, \"\n",
    "          f\"Val Acc: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader.dataset):.4f}, \"\n",
    "      f\"Test Acc: {(100 * correct / total):.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
