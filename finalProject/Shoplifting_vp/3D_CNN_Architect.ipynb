{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1r9Izz24SwQcrt9C-cqJ8m7tgaF1gokn_","authorship_tag":"ABX9TyP1m3LYgOpG51dFwXiLMGv8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torchinfo\n","!pip install av"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdNDkRTFnlg5","executionInfo":{"status":"ok","timestamp":1715509683538,"user_tz":-180,"elapsed":10114,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"5a7a97f4-0adb-4643-a1ce-7728ab497c04"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (12.0.0)\n"]}]},{"cell_type":"code","source":["# get data\n","!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/r3yjf35hzr-1.zip\n","!unzip r3yjf35hzr-1.zip -d ./\n","!mv 'Shoplifting Dataset (2022) - CV Laboratory MNNIT Allahabad' shopliftingdata\n","!unzip /content/shopliftingdata/Dataset.zip -d ./\n","!cd /content/Dataset\n","!pip install -U kora"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"oTkfN5Gb3UbP","executionInfo":{"status":"ok","timestamp":1715504220309,"user_tz":-180,"elapsed":95009,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"0e803d11-ccb5-44fc-e16f-a3128789e800"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-05-12 08:55:24--  https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/r3yjf35hzr-1.zip\n","Resolving prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com)... 3.5.65.69, 52.218.93.56, 3.5.66.161, ...\n","Connecting to prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com)|3.5.65.69|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 760777897 (726M) [application/zip]\n","Saving to: ‘r3yjf35hzr-1.zip’\n","\n","r3yjf35hzr-1.zip    100%[===================>] 725.53M  11.3MB/s    in 76s     \n","\n","2024-05-12 08:56:42 (9.52 MB/s) - ‘r3yjf35hzr-1.zip’ saved [760777897/760777897]\n","\n","Archive:  r3yjf35hzr-1.zip\n","  inflating: ./Shoplifting Dataset (2022) - CV Laboratory MNNIT Allahabad/Dataset.zip  \n","Archive:  /content/shopliftingdata/Dataset.zip\n","   creating: ./Dataset/\n","   creating: ./Dataset/Normal/\n","  inflating: ./Dataset/Normal/Normal (1).mp4  \n","  inflating: ./Dataset/Normal/Normal (10).mp4  \n","  inflating: ./Dataset/Normal/Normal (11).mp4  \n","  inflating: ./Dataset/Normal/Normal (12).mp4  \n","  inflating: ./Dataset/Normal/Normal (13).mp4  \n","  inflating: ./Dataset/Normal/Normal (14).mp4  \n","  inflating: ./Dataset/Normal/Normal (15).mp4  \n","  inflating: ./Dataset/Normal/Normal (16).mp4  \n","  inflating: ./Dataset/Normal/Normal (17).mp4  \n","  inflating: ./Dataset/Normal/Normal (18).mp4  \n","  inflating: ./Dataset/Normal/Normal (19).mp4  \n","  inflating: ./Dataset/Normal/Normal (2).mp4  \n","  inflating: ./Dataset/Normal/Normal (20).mp4  \n","  inflating: ./Dataset/Normal/Normal (21).mp4  \n","  inflating: ./Dataset/Normal/Normal (22).mp4  \n","  inflating: ./Dataset/Normal/Normal (23).mp4  \n","  inflating: ./Dataset/Normal/Normal (24).mp4  \n","  inflating: ./Dataset/Normal/Normal (25).mp4  \n","  inflating: ./Dataset/Normal/Normal (26).mp4  \n","  inflating: ./Dataset/Normal/Normal (27).mp4  \n","  inflating: ./Dataset/Normal/Normal (28).mp4  \n","  inflating: ./Dataset/Normal/Normal (29).mp4  \n","  inflating: ./Dataset/Normal/Normal (3).mp4  \n","  inflating: ./Dataset/Normal/Normal (30).mp4  \n","  inflating: ./Dataset/Normal/Normal (31).mp4  \n","  inflating: ./Dataset/Normal/Normal (32).mp4  \n","  inflating: ./Dataset/Normal/Normal (33).mp4  \n","  inflating: ./Dataset/Normal/Normal (34).mp4  \n","  inflating: ./Dataset/Normal/Normal (35).mp4  \n","  inflating: ./Dataset/Normal/Normal (36).mp4  \n","  inflating: ./Dataset/Normal/Normal (37).mp4  \n","  inflating: ./Dataset/Normal/Normal (38).mp4  \n","  inflating: ./Dataset/Normal/Normal (39).mp4  \n","  inflating: ./Dataset/Normal/Normal (4).mp4  \n","  inflating: ./Dataset/Normal/Normal (40).mp4  \n","  inflating: ./Dataset/Normal/Normal (41).mp4  \n","  inflating: ./Dataset/Normal/Normal (42).mp4  \n","  inflating: ./Dataset/Normal/Normal (43).mp4  \n","  inflating: ./Dataset/Normal/Normal (44).mp4  \n","  inflating: ./Dataset/Normal/Normal (45).mp4  \n","  inflating: ./Dataset/Normal/Normal (46).mp4  \n","  inflating: ./Dataset/Normal/Normal (47).mp4  \n","  inflating: ./Dataset/Normal/Normal (48).mp4  \n","  inflating: ./Dataset/Normal/Normal (49).mp4  \n","  inflating: ./Dataset/Normal/Normal (5).mp4  \n","  inflating: ./Dataset/Normal/Normal (50).mp4  \n","  inflating: ./Dataset/Normal/Normal (51).mp4  \n","  inflating: ./Dataset/Normal/Normal (52).mp4  \n","  inflating: ./Dataset/Normal/Normal (53).mp4  \n","  inflating: ./Dataset/Normal/Normal (54).mp4  \n","  inflating: ./Dataset/Normal/Normal (55).mp4  \n","  inflating: ./Dataset/Normal/Normal (56).mp4  \n","  inflating: ./Dataset/Normal/Normal (57).mp4  \n","  inflating: ./Dataset/Normal/Normal (58).mp4  \n","  inflating: ./Dataset/Normal/Normal (59).mp4  \n","  inflating: ./Dataset/Normal/Normal (6).mp4  \n","  inflating: ./Dataset/Normal/Normal (60).mp4  \n","  inflating: ./Dataset/Normal/Normal (61).mp4  \n","  inflating: ./Dataset/Normal/Normal (62).mp4  \n","  inflating: ./Dataset/Normal/Normal (63).mp4  \n","  inflating: ./Dataset/Normal/Normal (64).mp4  \n","  inflating: ./Dataset/Normal/Normal (65).mp4  \n","  inflating: ./Dataset/Normal/Normal (66).mp4  \n","  inflating: ./Dataset/Normal/Normal (67).mp4  \n","  inflating: ./Dataset/Normal/Normal (68).mp4  \n","  inflating: ./Dataset/Normal/Normal (69).mp4  \n","  inflating: ./Dataset/Normal/Normal (7).mp4  \n","  inflating: ./Dataset/Normal/Normal (70).mp4  \n","  inflating: ./Dataset/Normal/Normal (71).mp4  \n","  inflating: ./Dataset/Normal/Normal (72).mp4  \n","  inflating: ./Dataset/Normal/Normal (73).mp4  \n","  inflating: ./Dataset/Normal/Normal (74).mp4  \n","  inflating: ./Dataset/Normal/Normal (75).mp4  \n","  inflating: ./Dataset/Normal/Normal (76).mp4  \n","  inflating: ./Dataset/Normal/Normal (77).mp4  \n","  inflating: ./Dataset/Normal/Normal (78).mp4  \n","  inflating: ./Dataset/Normal/Normal (79).mp4  \n","  inflating: ./Dataset/Normal/Normal (8).mp4  \n","  inflating: ./Dataset/Normal/Normal (80).mp4  \n","  inflating: ./Dataset/Normal/Normal (81).mp4  \n","  inflating: ./Dataset/Normal/Normal (82).mp4  \n","  inflating: ./Dataset/Normal/Normal (83).mp4  \n","  inflating: ./Dataset/Normal/Normal (84).mp4  \n","  inflating: ./Dataset/Normal/Normal (85).mp4  \n","  inflating: ./Dataset/Normal/Normal (86).mp4  \n","  inflating: ./Dataset/Normal/Normal (87).mp4  \n","  inflating: ./Dataset/Normal/Normal (88).mp4  \n","  inflating: ./Dataset/Normal/Normal (89).mp4  \n","  inflating: ./Dataset/Normal/Normal (9).mp4  \n","  inflating: ./Dataset/Normal/Normal (90).mp4  \n","   creating: ./Dataset/Shoplifting/\n","  inflating: ./Dataset/Shoplifting/Shoplifting (1).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (10).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (11).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (12).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (13).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (14).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (15).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (16).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (17).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (18).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (19).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (2).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (20).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (21).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (22).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (23).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (24).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (25).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (26).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (27).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (28).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (29).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (3).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (30).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (31).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (32).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (33).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (34).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (35).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (36).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (37).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (38).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (39).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (4).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (40).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (41).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (42).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (43).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (44).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (45).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (46).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (47).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (48).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (5).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (50).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (51).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (52).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (53).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (54).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (55).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (56).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (57).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (58).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (59).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (6).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (60).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (61).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (62).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (63).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (64).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (65).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (66).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (67).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (68).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (69).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (7).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (70).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (71).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (72).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (73).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (74).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (75).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (76).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (77).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (78).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (79).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (8).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (80).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (81).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (82).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (83).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (84).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (85).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (86).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (87).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (88).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (89).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (9).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (90).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (91).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (92).mp4  \n","  inflating: ./Dataset/Shoplifting/Shoplifting (93).mp4  \n","Collecting kora\n","  Downloading kora-0.9.20-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kora) (7.34.0)\n","Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from kora) (1.5.33)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->kora) (24.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (67.7.2)\n","Collecting jedi>=0.16 (from ipython->kora)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kora) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kora) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kora) (0.2.13)\n","Installing collected packages: jedi, kora\n","Successfully installed jedi-0.19.1 kora-0.9.20\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Video Processing/finalProject/Shoplifting_vp')"],"metadata":{"id":"9kdmzTyloAkL","executionInfo":{"status":"ok","timestamp":1715504222288,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"s7HG0DQanQee","executionInfo":{"status":"ok","timestamp":1715504293480,"user_tz":-180,"elapsed":11274,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"outputs":[],"source":["# importların yapılması\n","import os\n","import numpy as np\n","import gc\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from copy import deepcopy\n","from torchinfo import summary\n","\n","import dataloader as dataloader\n","import utils as utils\n","\n","import time\n","import importlib"]},{"cell_type":"code","source":["importlib.reload(dataloader)\n","importlib.reload(utils)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"LvL_096InkO4","executionInfo":{"status":"ok","timestamp":1715507437803,"user_tz":-180,"elapsed":765,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IM_S5-5Yk0zR","executionInfo":{"status":"ok","timestamp":1715504580633,"user_tz":-180,"elapsed":610,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"19ca07b5-8820-4c14-95c7-f41614f86603"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["## initialize dataloader\n","\n","# Define your transform (data pre-processing) fınction\n","# Define your dataset with transform\n","transform = transforms.Compose([\n","    dataloader.ShopliftingPreprocessing(output_size=(120, 120))\n","])\n"],"metadata":{"id":"nf8V8VPOqwLs","executionInfo":{"status":"ok","timestamp":1715504582832,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Define your dataset\n","dataset = dataloader.ShopliftingDataLoader(root_dir='Dataset', transform=transform)"],"metadata":{"collapsed":true,"id":"FwzfKOeOq4OX","executionInfo":{"status":"ok","timestamp":1715504593069,"user_tz":-180,"elapsed":463,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["dataset.samples[35]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fqFupWdCtTq","executionInfo":{"status":"ok","timestamp":1715504772563,"user_tz":-180,"elapsed":551,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"43081433-2bd2-4b74-b5a7-fec72d7524e8"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('Dataset/Normal/Normal (41).mp4', 0)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["dataset.__getitem__(35)[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWFOFy0AEA_p","executionInfo":{"status":"ok","timestamp":1715505159107,"user_tz":-180,"elapsed":569,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"02854401-f0d2-4f1e-a2b8-7e8111308f8c"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 120, 120, 30)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Split the dataset\n","train_set, val_set, test_set = dataloader.split_dataset(dataset)"],"metadata":{"id":"duN1KLQA6H-_","executionInfo":{"status":"ok","timestamp":1715505167822,"user_tz":-180,"elapsed":554,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["len(train_set), len(val_set), len(test_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SggPQDwAE6hI","executionInfo":{"status":"ok","timestamp":1715505196120,"user_tz":-180,"elapsed":674,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"ccebcd5a-bfb6-4e4c-f4f5-2daf51f9b55f"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145, 18, 19)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Create data loaders\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=8, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False)"],"metadata":{"id":"4Y5flypn6JEB","executionInfo":{"status":"ok","timestamp":1715505200920,"user_tz":-180,"elapsed":3,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZF2eS6bHUUo8","executionInfo":{"status":"ok","timestamp":1715508109412,"user_tz":-180,"elapsed":1292,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"af411673-2d03-457d-da84-90a3a37ba6b6"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1982"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["class CNN3D(nn.Module):\n","    def __init__(self, t_dim=30, img_x=120, img_y=120, drop_p=0.4, fc_hidden1=1024, fc_hidden2=128, num_classes=2, send_device_fc=True):\n","        super(CNN3D, self).__init__()\n","\n","        # set video dimension\n","        self.t_dim = t_dim\n","        self.img_x = img_x\n","        self.img_y = img_y\n","        self.send_device_fc = send_device_fc\n","\n","        # fully connected layer hidden nodes\n","        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n","        self.drop_p = drop_p  # dropout probability\n","        self.num_classes = num_classes # Normal, Shoplifting\n","        self.ch1, self.ch2, self.ch3, self.ch4, self.ch5, self.ch6 = 8, 16, 32, 64, 96, 128\n","        self.k1, self.k2, self.k3, self.k4 = (3, 3, 3), (5, 5, 5), (7, 7, 7), (3, 1, 1)  # 3d kernel size\n","        self.s1, self.s2, self.s3 = (1, 1, 1), (2, 1, 1), (2, 2, 2)  # 3d strides\n","        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 1)  # 3d padding\n","\n","        # Layer 0 - Shape\n","        self.conv0_0_out = utils.conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)\n","        print(self.conv0_0_out)\n","        self.conv0_1_out = utils.conv3D_output_size(self.conv0_0_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_1_out)\n","        self.conv0_2_out = utils.conv3D_output_size(self.conv0_1_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_2_out)\n","        self.pool0_3_out = utils.pooling_output_size(self.conv0_2_out, self.s3, (1, 1, 1), (0, 0, 0))\n","        print(\"mp\")\n","        print(self.pool0_3_out)\n","        self.conv0_4_out = utils.conv3D_output_size(self.pool0_3_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_4_out)\n","        self.pool0_5_out = utils.pooling_output_size(self.conv0_4_out, self.s2, (1, 1, 1), (0, 0, 0))\n","        print(\"mp\")\n","        print(self.pool0_5_out)\n","        self.conv0_6_out = utils.conv3D_output_size(self.pool0_5_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_6_out)\n","        self.conv0_7_out = utils.conv3D_output_size(self.conv0_6_out, self.pd1, self.k2, self.s1)\n","        print(self.conv0_7_out)\n","        self.conv0_8_out = utils.conv3D_output_size(self.conv0_7_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_8_out)\n","        self.conv0_9_out = utils.conv3D_output_size(self.conv0_8_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_9_out)\n","\n","        # Layer 0 - Shape Cont.\n","        self.conv0_12_out = utils.conv3D_output_size(self.conv0_9_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_12_out)\n","        self.conv0_13_out = utils.conv3D_output_size(self.conv0_12_out, self.pd1, self.k1, self.s1)\n","        print(self.conv0_13_out)\n","        self.pool0_14_out = utils.pooling_output_size(self.conv0_13_out, self.s3, (1, 1, 1), (0, 0, 0))\n","        print(\"mp\")\n","        print(self.pool0_14_out)\n","\n","        # Final Layer - Shape\n","        self.conv3_2_out = utils.conv3D_output_size(self.pool0_14_out, self.pd1, self.k1, self.s1)\n","        print(self.conv3_2_out)\n","        self.conv3_3_out = utils.conv3D_output_size(self.conv3_2_out, self.pd1, self.k1, self.s1)\n","        print(self.conv3_3_out)\n","\n","        # Layer 0\n","        self.conv0_0 = nn.Conv3d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","\n","        self.conv0_1 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","\n","        self.conv0_2 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch2, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.pool0_3 = nn.MaxPool3d(kernel_size = (2, 2, 2), stride = (1, 1, 1))\n","        self.conv0_4 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch2, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv0_4_2 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.pool0_5 = nn.MaxPool3d(kernel_size = (2, 1, 1), stride = (2, 1, 1)) # start point of parallel layers\n","        self.conv0_6 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv0_7 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch3, kernel_size=self.k2, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv0_8 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch3, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv0_9 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch3, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        print(\"Layer 0\")\n","        # 1st parallel layer\n","        self.conv1_0 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv1_1 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch3, kernel_size=self.k3, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv1_2 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch3, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","\n","        print(\"Layer 1\")\n","        # first concatination and batch normalisation\n","        self.bn0_11 = nn.BatchNorm3d(self.ch4)\n","        self.conv0_12 = nn.Conv3d(in_channels=self.ch4, out_channels=self.ch4, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv0_13 = nn.Conv3d(in_channels=self.ch4, out_channels=self.ch4, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.pool0_14 = nn.MaxPool3d(kernel_size = (2, 2, 2), stride = (2, 2, 2))\n","        print(\"First Concat\")\n","\n","        # 2nd parallel layer\n","        self.conv2_0 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch3, kernel_size=self.k1, stride=self.s3,\n","                               padding=self.pd1)\n","        self.pool2_1 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1))\n","        self.conv2_2 = nn.Conv3d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.conv2_3 = nn.Conv3d(in_channels=self.ch4, out_channels=self.ch4, kernel_size=self.k1, stride=self.s2,\n","                               padding=self.pd1)\n","        self.conv2_4 = nn.Conv3d(in_channels=self.ch4, out_channels=self.ch4, kernel_size=self.k4, stride=self.s2,\n","                               padding=self.pd1)\n","        self.pool2_5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1))\n","        print(\"2nd Parallel\")\n","\n","        # last sequential layer\n","        self.bn3_1 = nn.BatchNorm3d(self.ch4)\n","        self.conv3_2 = nn.Conv3d(in_channels=self.ch4, out_channels=self.ch5, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd2) # need to keep 3 time_dim\n","        self.conv3_3 = nn.Conv3d(in_channels=self.ch5, out_channels=self.ch5, kernel_size=self.k1, stride=self.s1,\n","                               padding=self.pd1)\n","        self.bn3_4 = nn.BatchNorm3d(self.ch5)\n","        print(\"Last Sequential\")\n","\n","        # Fully connected layes\n","        print(self.ch5)\n","        print(self.conv3_3_out[0])\n","        print(self.conv3_3_out[1])\n","        print(self.conv3_3_out[2])\n","\n","        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n","        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes) # fully connected layer, output = binary_classes\n","\n","    def forward(self, x_3d):\n","        # Layer 0\n","        a = self.conv0_0(x_3d)\n","        print(a.size())\n","        a = self.conv0_1(a)\n","        print(a.size())\n","        a = self.conv0_2(a)\n","        print(a.size())\n","        print(\"mp\")\n","        a = self.pool0_3(a)\n","        print(a.size())\n","        a = self.conv0_4(a)\n","        print(a.size())\n","        c = self.conv0_4_2(a)\n","        print(c.size())\n","        a = self.pool0_5(a)\n","        print(\"mp\")\n","        print(a.size())\n","        b = a\n","        a = self.conv0_6(a)\n","        print(a.size())\n","        a = self.conv0_7(a)\n","        print(a.size())\n","        a = self.conv0_8(a)\n","        print(a.size())\n","        a = self.conv0_9(a)\n","        print(a.size())\n","\n","        # Layer 1\n","        print(\"b\")\n","        b = self.conv1_0(b)\n","        print(b.size())\n","        b = self.conv1_1(b)\n","        print(b.size())\n","        b = self.conv1_2(b)\n","        print(b.size())\n","\n","        # Layer 0 - 1 merge\n","        print(\"ab\")\n","        ab = torch.cat([a, b], dim=1)\n","        print(ab.size())\n","        ab = self.bn0_11(ab)\n","        print(ab.size())\n","        ab = self.conv0_12(ab)\n","        print(ab.size())\n","        ab = self.conv0_13(ab)\n","        print(ab.size())\n","        ab = self.pool0_14(ab)\n","        print(ab.size())\n","\n","        # Layer 2\n","        print(\"c\")\n","        c = self.conv2_0(c)\n","        print(c.size())\n","        c = self.pool2_1(c)\n","        print(c.size())\n","        c = self.conv2_2(c)\n","        print(c.size())\n","        c = self.conv2_3(c)\n","        print(c.size())\n","        c = self.conv2_4(c)\n","        print(c.size())\n","        c = self.pool2_5(c)\n","        print(c.size())\n","\n","        # Layer 0 - 1 - 2 merge\n","        print(\"abc\")\n","        abc = torch.cat([ab, c], dim=2)\n","        print(abc.size())\n","        abc = self.bn3_1(abc)\n","        print(abc.size())\n","        abc = self.conv3_2(abc)\n","        print(abc.size())\n","        abc = self.conv3_3(abc)\n","        print(abc.size())\n","        abc = self.bn3_4(abc)\n","        print(abc.size())\n","\n","\n","        # Dropout & fully connected layers\n","        abc = abc.view(abc.size(0), -1)\n","        print(abc.size())\n","        abc = F.dropout(abc, p=self.drop_p, training=self.training)\n","        print(abc.size()[1])\n","        if self.send_device_fc: # Handler for GPU-CPU confusion\n","          fc1 = nn.Linear(abc.size()[1], self.fc_hidden1) # fully connected hidden layer\n","          fc1.to(device)\n","          abc = fc1(abc)\n","        else:\n","          abc = nn.Linear(abc.size()[1], self.fc_hidden1)(abc)  # fully connected hidden layer\n","\n","        print(abc.size())\n","        abc = F.dropout(abc, p=self.drop_p, training=self.training)\n","        print(abc.size())\n","        abc = self.fc2(abc)\n","        print(abc.size())\n","        abc = F.dropout(abc, p=self.drop_p, training=self.training)\n","        print(abc.size())\n","        abc = self.fc3(abc)\n","        print(abc.size())\n","        output = F.softmax(abc, dim=1)\n","        print(output.size())\n","\n","        return output"],"metadata":{"id":"qEA70b-3nkYx","executionInfo":{"status":"ok","timestamp":1715509376236,"user_tz":-180,"elapsed":468,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Let's create an instance of the model and print its architecture\n","model = CNN3D(t_dim=30, img_x=120, img_y=120, drop_p=0.4, fc_hidden1=1024, fc_hidden2=128, num_classes=2, send_device_fc=False)\n","print(model)\n","\n","# Define the shape of the dummy input data (batch_size, channels, frames, height, width)\n","dummy_input_shape = (32, 3, 120, 120, 30) # Batch size 32, 3 input channels, 16 frames, 112x112 resolution\n","\n","# Generate random dummy input data within the defined shape\n","dummy_input = torch.randn(*dummy_input_shape)\n","\n","# Forward pass through the model\n","output = model(dummy_input)\n","\n","# Print the output shape\n","print(\"Output shape:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P52N0spwhwas","executionInfo":{"status":"ok","timestamp":1715509399789,"user_tz":-180,"elapsed":19646,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"d7d0056f-dcc0-4593-9791-2d10a5a31656"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["(28, 118, 118)\n","(26, 116, 116)\n","(24, 114, 114)\n","mp\n","(23, 113, 113)\n","(21, 111, 111)\n","mp\n","(20, 111, 111)\n","(18, 109, 109)\n","(14, 105, 105)\n","(12, 103, 103)\n","(10, 101, 101)\n","(8, 99, 99)\n","(6, 97, 97)\n","mp\n","(5, 96, 96)\n","(3, 94, 94)\n","(1, 92, 92)\n","Layer 0\n","Layer 1\n","First Concat\n","2nd Parallel\n","Last Sequential\n","96\n","1\n","92\n","92\n","CNN3D(\n","  (conv0_0): Conv3d(3, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_1): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (pool0_3): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (conv0_4): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_4_2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (pool0_5): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (conv0_6): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_7): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1))\n","  (conv0_8): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_9): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv1_0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv1_1): Conv3d(32, 32, kernel_size=(7, 7, 7), stride=(1, 1, 1))\n","  (conv1_2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (bn0_11): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv0_12): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_13): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (pool0_14): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (conv2_0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n","  (pool2_1): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (conv2_2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv2_3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 1, 1))\n","  (conv2_4): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(2, 1, 1))\n","  (pool2_5): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (bn3_1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3_2): Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 0, 1))\n","  (conv3_3): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (bn3_4): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",")\n","torch.Size([32, 8, 118, 118, 28])\n","torch.Size([32, 16, 116, 116, 26])\n","torch.Size([32, 16, 114, 114, 24])\n","mp\n","torch.Size([32, 16, 113, 113, 23])\n","torch.Size([32, 16, 111, 111, 21])\n","torch.Size([32, 32, 109, 109, 19])\n","mp\n","torch.Size([32, 16, 55, 111, 21])\n","torch.Size([32, 32, 53, 109, 19])\n","torch.Size([32, 32, 49, 105, 15])\n","torch.Size([32, 32, 47, 103, 13])\n","torch.Size([32, 32, 45, 101, 11])\n","b\n","torch.Size([32, 32, 53, 109, 19])\n","torch.Size([32, 32, 47, 103, 13])\n","torch.Size([32, 32, 45, 101, 11])\n","ab\n","torch.Size([32, 64, 45, 101, 11])\n","torch.Size([32, 64, 45, 101, 11])\n","torch.Size([32, 64, 43, 99, 9])\n","torch.Size([32, 64, 41, 97, 7])\n","torch.Size([32, 64, 20, 48, 3])\n","c\n","torch.Size([32, 32, 54, 54, 9])\n","torch.Size([32, 32, 53, 53, 8])\n","torch.Size([32, 64, 51, 51, 6])\n","torch.Size([32, 64, 25, 49, 4])\n","torch.Size([32, 64, 12, 49, 4])\n","torch.Size([32, 64, 11, 48, 3])\n","abc\n","torch.Size([32, 64, 31, 48, 3])\n","torch.Size([32, 64, 31, 48, 3])\n","torch.Size([32, 96, 29, 46, 3])\n","torch.Size([32, 96, 27, 44, 1])\n","torch.Size([32, 96, 27, 44, 1])\n","torch.Size([32, 114048])\n","114048\n","torch.Size([32, 1024])\n","torch.Size([32, 1024])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32, 2])\n","torch.Size([32, 2])\n","Output shape: torch.Size([32, 2])\n"]}]},{"cell_type":"code","source":["summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_pk6TYaX-lc","executionInfo":{"status":"ok","timestamp":1715508097115,"user_tz":-180,"elapsed":612,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"22f555df-401f-4c20-f64f-f270e8ed9be4"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["=================================================================\n","Layer (type:depth-idx)                   Param #\n","=================================================================\n","CNN3D                                    --\n","├─Conv3d: 1-1                            656\n","├─Conv3d: 1-2                            3,472\n","├─Conv3d: 1-3                            6,928\n","├─MaxPool3d: 1-4                         --\n","├─Conv3d: 1-5                            6,928\n","├─Conv3d: 1-6                            13,856\n","├─MaxPool3d: 1-7                         --\n","├─Conv3d: 1-8                            13,856\n","├─Conv3d: 1-9                            128,032\n","├─Conv3d: 1-10                           27,680\n","├─Conv3d: 1-11                           27,680\n","├─Conv3d: 1-12                           13,856\n","├─Conv3d: 1-13                           351,264\n","├─Conv3d: 1-14                           27,680\n","├─BatchNorm3d: 1-15                      128\n","├─Conv3d: 1-16                           110,656\n","├─Conv3d: 1-17                           110,656\n","├─MaxPool3d: 1-18                        --\n","├─Conv3d: 1-19                           27,680\n","├─MaxPool3d: 1-20                        --\n","├─Conv3d: 1-21                           55,360\n","├─Conv3d: 1-22                           110,656\n","├─Conv3d: 1-23                           12,352\n","├─MaxPool3d: 1-24                        --\n","├─BatchNorm3d: 1-25                      128\n","├─Conv3d: 1-26                           165,984\n","├─Conv3d: 1-27                           248,928\n","├─BatchNorm3d: 1-28                      192\n","├─Linear: 1-29                           131,200\n","├─Linear: 1-30                           258\n","=================================================================\n","Total params: 1,596,066\n","Trainable params: 1,596,066\n","Non-trainable params: 0\n","================================================================="]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["model = CNN3D(t_dim=30, img_x=112, img_y=112, drop_p=0.4, fc_hidden1=1024, fc_hidden2=128, num_classes=2, send_device_fc=True)"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"cWS8uDjXrA7e","executionInfo":{"status":"ok","timestamp":1715465014706,"user_tz":-180,"elapsed":321,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"2cc0681d-cabd-4acb-9c2e-f25eaa696f82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(28, 110, 110)\n","(26, 108, 108)\n","(24, 106, 106)\n","(23, 105, 105)\n","(21, 103, 103)\n","(20, 103, 103)\n","(18, 101, 101)\n","(14, 97, 97)\n","(12, 95, 95)\n","(10, 93, 93)\n","(8, 91, 91)\n","(6, 89, 89)\n","(5, 88, 88)\n","(3, 86, 86)\n","(1, 84, 84)\n","Layer 0\n","Layer 1\n","First Concat\n","2nd Parallel\n","Last Sequential\n","96\n","1\n","84\n","84\n"]}]},{"cell_type":"code","source":["# hyper-params\n","num_epochs = 100\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# dont forget to send model to device\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Csx8PMWnLRl","executionInfo":{"status":"ok","timestamp":1715469265211,"user_tz":-180,"elapsed":316,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"056dc34f-c470-4096-f30c-a63738973812"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN3D(\n","  (conv0_0): Conv3d(3, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_1): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (pool0_3): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (conv0_4): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_4_2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (pool0_5): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (conv0_6): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_7): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1))\n","  (conv0_8): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_9): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv1_0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv1_1): Conv3d(32, 32, kernel_size=(7, 7, 7), stride=(1, 1, 1))\n","  (conv1_2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (bn0_11): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv0_12): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv0_13): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (pool0_14): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (conv2_0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n","  (pool2_1): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (conv2_2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (conv2_3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 1, 1))\n","  (conv2_4): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(2, 1, 1))\n","  (pool2_5): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","  (bn3_1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3_2): Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 0, 1))\n","  (conv3_3): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n","  (bn3_4): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["# Training loop\n","best_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        print(outputs)\n","        print(targets)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            val_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    if (100 * correct / total) > best_acc:\n","        best_acc = 100 * correct / total\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n","          f\"Train Loss: {running_loss / len(train_loader.dataset):.4f}, \"\n","          f\"Val Loss: {val_loss / len(val_loader.dataset):.4f}, \"\n","          f\"Val Acc: {(100 * correct / total):.2f}%\")\n","\n","# Testing loop\n","model.eval()\n","test_loss = 0.0\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, targets in test_loader:\n","        inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device)\n","        print(outputs)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        test_loss += loss.item() * inputs.size(0)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","print(f\"Test Loss: {test_loss / len(test_loader.dataset):.4f}, \"\n","      f\"Test Acc: {(100 * correct / total):.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"collapsed":true,"id":"ElcnaiPvi1bh","executionInfo":{"status":"error","timestamp":1715469501197,"user_tz":-180,"elapsed":13090,"user":{"displayName":"Anıl Özcan","userId":"06510389857538456980"}},"outputId":"d88a4efd-9bf9-4cb9-d68d-f734209e3dc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n","  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[0.7868, 0.2132],\n","        [0.6745, 0.3255],\n","        [0.8398, 0.1602],\n","        [0.8308, 0.1692],\n","        [0.7429, 0.2571],\n","        [0.7350, 0.2650],\n","        [0.4778, 0.5222],\n","        [0.5581, 0.4419],\n","        [0.7224, 0.2776],\n","        [0.6286, 0.3714],\n","        [0.7884, 0.2116],\n","        [0.6537, 0.3463],\n","        [0.5347, 0.4653],\n","        [0.6219, 0.3781],\n","        [0.8091, 0.1909],\n","        [0.7326, 0.2674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')\n"]},{"output_type":"error","ename":"ValueError","evalue":"Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 2]))","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-8614ce654625>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    726\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 2]))"]}]}]}